{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Homework 1"
      ],
      "metadata": {
        "id": "tFaf7VErZ1Bf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Installing packages"
      ],
      "metadata": {
        "id": "6vsESFZylO6O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install langchain_google_genai\n",
        "!pip install langchain-openai"
      ],
      "metadata": {
        "id": "Hrdfpmv9nMpw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f36e4d74-e83a-4637-ded6-190085f155a3",
        "collapsed": true
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain-openai in /usr/local/lib/python3.12/dist-packages (1.1.7)\n",
            "Requirement already satisfied: langchain-core<2.0.0,>=1.2.6 in /usr/local/lib/python3.12/dist-packages (from langchain-openai) (1.2.7)\n",
            "Requirement already satisfied: openai<3.0.0,>=1.109.1 in /usr/local/lib/python3.12/dist-packages (from langchain-openai) (2.15.0)\n",
            "Requirement already satisfied: tiktoken<1.0.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from langchain-openai) (0.12.0)\n",
            "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.6->langchain-openai) (1.33)\n",
            "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.6->langchain-openai) (0.6.4)\n",
            "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.6->langchain-openai) (25.0)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.6->langchain-openai) (2.12.3)\n",
            "Requirement already satisfied: pyyaml<7.0.0,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.6->langchain-openai) (6.0.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.6->langchain-openai) (9.1.2)\n",
            "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.6->langchain-openai) (4.15.0)\n",
            "Requirement already satisfied: uuid-utils<1.0,>=0.12.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.6->langchain-openai) (0.13.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.109.1->langchain-openai) (4.12.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.109.1->langchain-openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.109.1->langchain-openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.10.0 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.109.1->langchain-openai) (0.12.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.109.1->langchain-openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.109.1->langchain-openai) (4.67.1)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.12/dist-packages (from tiktoken<1.0.0,>=0.7.0->langchain-openai) (2025.11.3)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.12/dist-packages (from tiktoken<1.0.0,>=0.7.0->langchain-openai) (2.32.4)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->openai<3.0.0,>=1.109.1->langchain-openai) (3.11)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai<3.0.0,>=1.109.1->langchain-openai) (2026.1.4)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai<3.0.0,>=1.109.1->langchain-openai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai<3.0.0,>=1.109.1->langchain-openai) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.2.6->langchain-openai) (3.0.0)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.6->langchain-openai) (3.11.5)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.6->langchain-openai) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.6->langchain-openai) (0.25.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<2.0.0,>=1.2.6->langchain-openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<2.0.0,>=1.2.6->langchain-openai) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<2.0.0,>=1.2.6->langchain-openai) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken<1.0.0,>=0.7.0->langchain-openai) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken<1.0.0,>=0.7.0->langchain-openai) (2.5.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Downloading receipts.zip\n",
        "The codes below download and unzip receipts.zip from Google Drive. receipts.zip contains all images from the Fusion folder on BlackBoard.\n"
      ],
      "metadata": {
        "id": "kCENjOq6owDd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gdown\n",
        "file_id = \"1oe2FZd3ZTO7nrDqjCafNvxicl08oF8JF\"\n",
        "download_url = f\"https://drive.google.com/uc?id={file_id}\"\n",
        "gdown.download(download_url, \"receipts.zip\", quiet=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 110
        },
        "id": "1kCCp8DwPF4L",
        "outputId": "41b8c3c3-df05-4e93-b437-8f475b6d5545"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1oe2FZd3ZTO7nrDqjCafNvxicl08oF8JF\n",
            "To: /content/receipts.zip\n",
            "100%|██████████| 1.61M/1.61M [00:00<00:00, 20.6MB/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'receipts.zip'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip receipts.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YgWUBbITPvI3",
        "outputId": "3bd3974e-3edc-4b0f-b419-0b630ad10863"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Archive:  receipts.zip\n",
            "replace receipt1.jpg? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Helper functions\n",
        "\n",
        "We need two functions\n",
        "* image_to_base64 convert your jpg image into Base64 encoded string (basically a sequence of 64 characters to make your image easily transfered via API)\n",
        "* get_image_data_url takes your jpg image, converting them into base64 string and construct a suitable input for GEMINI api call."
      ],
      "metadata": {
        "id": "PMY8D1EMpFPF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import base64\n",
        "import mimetypes\n",
        "\n",
        "# Helper function to read and encode image\n",
        "def image_to_base64(img_path):\n",
        "    with open(img_path, \"rb\") as img_file:\n",
        "        return base64.b64encode(img_file.read()).decode('utf-8')\n",
        "\n",
        "# Helper function to encode local file to Base64 Data URL\n",
        "def get_image_data_url(image_path):\n",
        "    # Guess the mime type (e.g., image/png, image/jpeg) based on file extension\n",
        "    mime_type, _ = mimetypes.guess_type(image_path)\n",
        "    if mime_type is None:\n",
        "        mime_type = \"image/png\" # Default fallback\n",
        "\n",
        "    encoded_string = image_to_base64(image_path)\n",
        "\n",
        "    # Construct the Data URL\n",
        "    return f\"data:{mime_type};base64,{encoded_string}\""
      ],
      "metadata": {
        "id": "8RjaDimUOg1p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "llm = ChatOpenAI(\n",
        "    model=\"gemini-2.5-flash\",\n",
        "    api_key=\"sk-e2GpF64Q21z5Ha1qt3Eg6NJiiBJrhFuf5s4rhr6nH9M78OWR\",\n",
        "    base_url=\"https://api.chatanywhere.tech/v1\",\n",
        "    temperature=0\n",
        ")"
      ],
      "metadata": {
        "id": "fSqh82PxPS4_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Display jpg images. Alternatively, open the folder icon on the left pannel to see the images."
      ],
      "metadata": {
        "id": "twGMWnQKAoSx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import HTML, display\n",
        "import glob, os\n",
        "\n",
        "image_paths = glob.glob(\"*.jpg\")\n",
        "image_paths.sort()\n",
        "html_content = '<div style=\"display: flex; flex-wrap: wrap; gap: 20px;\">'\n",
        "\n",
        "for path in image_paths:\n",
        "    b64 = image_to_base64(path)\n",
        "    filename = os.path.basename(path) # Clean up path to show just the name\n",
        "\n",
        "    # Create a vertical column for each image + text\n",
        "    html_content += f'''\n",
        "    <div style=\"display: flex; flex-direction: column; align-items: center;\">\n",
        "        <img src=\"data:image/jpeg;base64,{b64}\" style=\"height: 300px; border: 1px solid #ddd; margin-bottom: 5px;\"/>\n",
        "        <span style=\"font-family: monospace; font-size: 14px;\">{filename}</span>\n",
        "    </div>\n",
        "    '''\n",
        "\n",
        "html_content += '</div>'\n",
        "\n",
        "display(HTML(html_content))"
      ],
      "metadata": {
        "id": "vgg-TjwIRsIi",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Image input to Gemini\n",
        "Different from text, image needs to be converted into base64 encoded string and then formated into url before inputting to the language model. This is convenient for image-type input to be transfered through the API.\n",
        "\n",
        "You can find out more appropriate format for image-type data in this [Link](https://docs.langchain.com/oss/python/langchain/messages)\n",
        "\n"
      ],
      "metadata": {
        "id": "uZAWWr8Jo1-P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"You are a helpful assistant.\"),\n",
        "    (\"human\", [\n",
        "        {\"type\": \"text\", \"text\": \"{question}\"},\n",
        "        {\"type\": \"image_url\", \"image_url\": {\"url\": \"{image_url1}\"}},\n",
        "        {\"type\": \"image_url\", \"image_url\": {\"url\": \"{image_url2}\"}},\n",
        "    ]),\n",
        "])\n",
        "\n",
        "chain = prompt | llm\n",
        "\n",
        "image_path = \"/content/receipt1.jpg\"\n",
        "image_data_url = get_image_data_url(image_path)\n",
        "\n",
        "image_path2 = \"/content/receipt2.jpg\"\n",
        "image_data_url2 = get_image_data_url(image_path2)\n",
        "\n",
        "response = chain.invoke({\n",
        "    \"question\": \"What is in this picture?\",\n",
        "    \"image_url1\": image_data_url,\n",
        "    \"image_url2\": image_data_url2\n",
        "})\n",
        "\n",
        "print(response.content)"
      ],
      "metadata": {
        "id": "sVsD6duEm0d2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.messages import HumanMessage, SystemMessage\n",
        "\n",
        "# 初始化模型\n",
        "llm = ChatOpenAI(\n",
        "    model=\"gemini-2.5-flash\",\n",
        "    api_key=\"sk-e2GpF64Q21z5Ha1qt3Eg6NJiiBJrhFuf5s4rhr6nH9M78OWR\",\n",
        "    base_url=\"https://api.chatanywhere.tech/v1\",\n",
        "    temperature=0\n",
        ")\n",
        "\n",
        "def get_single_receipt_data(image_path):\n",
        "    \"\"\"提取单张收据的核心金额\"\"\"\n",
        "    system_msg = \"\"\"You are a receipt parser. Return ONLY a JSON object with:\n",
        "    {\n",
        "        \"total_spent\": float (How much money did I spend in total for these bills?),\n",
        "        \"original_price\": float (How much would I have had to pay without the discount)\n",
        "    }\n",
        "    Think step by step.\n",
        "    If the image is not a receipt, return {\"error\": \"REJECTED\"}.\n",
        "    \"\"\"\n",
        "\n",
        "    img_data = get_image_data_url(image_path)\n",
        "\n",
        "    messages = [\n",
        "        SystemMessage(content=system_msg),\n",
        "        HumanMessage(content=[\n",
        "            {\"type\": \"image_url\", \"image_url\": {\"url\": img_data}}\n",
        "        ])\n",
        "    ]\n",
        "\n",
        "    response = llm.invoke(messages)\n",
        "    try:\n",
        "        # 解析模型返回的 JSON 字符串\n",
        "        return json.loads(response.content.replace(\"```json\", \"\").replace(\"```\", \"\"))\n",
        "    except:\n",
        "        return None"
      ],
      "metadata": {
        "id": "BHO-271Pvsh0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_paths = [f\"/content/receipt{i}.jpg\" for i in range(1, 8)]\n",
        "\n",
        "query1_list = []\n",
        "query2_list = []\n",
        "\n",
        "print(\"--- 开始提取 7 张收据数据 ---\")\n",
        "for i, path in enumerate(image_paths):\n",
        "    data = get_single_receipt_data(path)\n",
        "    if data and \"total_spent\" in data:\n",
        "        query1_list.append(data[\"total_spent\"])\n",
        "        query2_list.append(data[\"original_price\"])\n",
        "        print(f\"收据 {i+1}: 实付 ${data['total_spent']}, 原价 ${data['original_price']}\")\n",
        "    else:\n",
        "        print(f\"收据 {i+1}: 识别失败或非收据格式\")\n",
        "\n",
        "# --- 最终计算结果给测试函数 ---\n",
        "query1_answer = sum(query1_list)\n",
        "query2_answer = sum(query2_list)\n",
        "\n",
        "print(\"-\" * 30)\n",
        "print(f\"总计实付 (Query 1): {query1_answer}\")\n",
        "print(f\"总计原价 (Query 2): {query2_answer}\")"
      ],
      "metadata": {
        "id": "rzgP6wM2vxCG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Evaluation Code\n",
        "\n",
        "* Make sure your LLM return a single float as the answer, stored in `query1_answer` and `query2_answer`\n",
        "* Run the following code blocks: (1) If the blocks does not return any error, then your chain design is correct. Otherwise, please check your chain design.\n",
        "\n",
        "* Do not modify `query_1_costs` and `query_2_costs`"
      ],
      "metadata": {
        "id": "rI2MF3_85UHN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def test_query(answer, ground_truth_costs):\n",
        "    # Convert string to float if necessary\n",
        "    if isinstance(answer, str):\n",
        "        answer = float(answer)\n",
        "\n",
        "    # Calculate the ground truth sum once for clarity\n",
        "    expected_total = sum(ground_truth_costs)\n",
        "\n",
        "    # Check if the answer is within +/- $2 of the expected total\n",
        "    assert abs(answer - expected_total) <= 2"
      ],
      "metadata": {
        "id": "SHI1QVEw5Toi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run the following code block to evaluate query 1:\n",
        "> How much money did I spend in total for these bills?"
      ],
      "metadata": {
        "id": "cQIXxTI__zgU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query_1_costs = [394.7, 316.1, 140.8, 514.0, 102.3, 190.8, 315.6] # do not modify this\n",
        "query1_answer = sum(query1_list)\n",
        "test_query(query1_answer, query_1_costs)"
      ],
      "metadata": {
        "id": "S-4zO_me_wt9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run the following code block to evaluate query 2:\n",
        "> How much would I have had to pay without the discount?"
      ],
      "metadata": {
        "id": "z5fs0-qpAAAo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query_2_costs = [480.20, 392.20, 160.10, 590.80, 107.70, 221.20, 396.00] # do not modify this\n",
        "query2_answer = sum(query2_list)\n",
        "test_query(query2_answer, query_2_costs)"
      ],
      "metadata": {
        "id": "5EvCKTbWADgA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sum([480.20, 392.20, 160.10, 590.80, 107.70, 221.20, 396.00])"
      ],
      "metadata": {
        "id": "h6ZRGssKHOw-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.messages import HumanMessage, SystemMessage\n",
        "\n",
        "# 初始化模型\n",
        "llm = ChatOpenAI(\n",
        "    model=\"gemini-2.5-flash\",\n",
        "    api_key=\"sk-e2GpF64Q21z5Ha1qt3Eg6NJiiBJrhFuf5s4rhr6nH9M78OWR\",\n",
        "    base_url=\"https://api.chatanywhere.tech/v1\",\n",
        "    temperature=0\n",
        ")\n",
        "\n",
        "def get_single_receipt_data(image_path):\n",
        "    \"\"\"提取单张收据的核心金额\"\"\"\n",
        "    system_msg = \"\"\"You are a receipt parser. Return ONLY a JSON object with:\n",
        "    {\n",
        "        \"total_spent\": float (How much money did I spend in total for these bills?),\n",
        "        \"original_price\": float (How much would I have had to pay without the discount)\n",
        "    }\n",
        "    Think step by step.\n",
        "    If the image is not a receipt, return {\"error\": \"REJECTED\"}.\n",
        "    \"\"\"\n",
        "\n",
        "    img_data = get_image_data_url(image_path)\n",
        "\n",
        "    messages = [\n",
        "        SystemMessage(content=system_msg),\n",
        "        HumanMessage(content=[\n",
        "            {\"type\": \"image_url\", \"image_url\": {\"url\": img_data}}\n",
        "        ])\n",
        "    ]\n",
        "\n",
        "    # 增加错误处理，防止某张图识别失败导致程序崩溃\n",
        "    try:\n",
        "        response = llm.invoke(messages)\n",
        "        content = response.content.replace(\"```json\", \"\").replace(\"```\", \"\")\n",
        "        return json.loads(content)\n",
        "    except:\n",
        "        return None\n",
        "\n",
        "def decide(query):\n",
        "    \"\"\"判断意图\"\"\"\n",
        "\n",
        "    system_msg = \"\"\"You are a query classifier.\n",
        "    Analyze the user's query and classify it into one of these integers:\n",
        "    1: The user asks about \"Total Spent\", \"Final Cost\", or \"How much paid\".\n",
        "    2: The user asks about \"Original Price\", \"Before Discount\", or \"Price without savings\".\n",
        "    0: The user asks anything else (Irrelevant query).\n",
        "\n",
        "    Return ONLY a JSON object: {\"class\": int}\n",
        "    \"\"\"\n",
        "\n",
        "    messages = [\n",
        "        SystemMessage(content=system_msg),\n",
        "        HumanMessage(content=query) # 2. 修正：必须把用户的问题传进去！\n",
        "    ]\n",
        "\n",
        "    try:\n",
        "        response = llm.invoke(messages)\n",
        "        content = response.content.replace(\"```json\", \"\").replace(\"```\", \"\")\n",
        "        result = json.loads(content)\n",
        "        return result.get(\"class\", 0) # 3. 修正：直接返回整数\n",
        "    except:\n",
        "        return 0 # 解析失败默认拒绝\n",
        "\n",
        "def llmchat(query, images):\n",
        "    # --- 第一步：判断意图 ---\n",
        "    intent_class = decide(query)\n",
        "    print(f\"意图类别: {intent_class}\")\n",
        "\n",
        "    # --- 第二步：拒绝逻辑 (符合 Source 49) ---\n",
        "    if intent_class == 0:\n",
        "        print(\"Reject! Irrelevant query.\")\n",
        "        return \"I can only answer questions about receipt totals.\" # 直接返回，不要继续跑图片循环\n",
        "\n",
        "    # --- 第三步：如果意图有效，才开始处理图片 ---\n",
        "    query1_list = []\n",
        "    query2_list = []\n",
        "\n",
        "    print(f\"--- 开始提取 {len(images)} 张收据数据 ---\")\n",
        "    for i, path in enumerate(images):\n",
        "        data = get_single_receipt_data(path)\n",
        "        if data and \"total_spent\" in data:\n",
        "            # 确保数据是数字\n",
        "            t_spent = data.get(\"total_spent\", 0.0)\n",
        "            o_price = data.get(\"original_price\", 0.0)\n",
        "\n",
        "            if isinstance(t_spent, (int, float)): query1_list.append(t_spent)\n",
        "            if isinstance(o_price, (int, float)): query2_list.append(o_price)\n",
        "\n",
        "            print(f\"收据 {i+1}: 实付 ${t_spent}, 原价 ${o_price}\")\n",
        "        else:\n",
        "            print(f\"收据 {i+1}: 识别失败\")\n",
        "\n",
        "    # --- 第四步：根据意图返回结果 ---\n",
        "    query1_answer = sum(query1_list)\n",
        "    query2_answer = sum(query2_list)\n",
        "\n",
        "    if intent_class == 1:\n",
        "        return query1_answer\n",
        "    elif intent_class == 2:\n",
        "        return query2_answer"
      ],
      "metadata": {
        "id": "cWzduEpGMp6n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_paths = [f\"/content/receipt{i}.jpg\" for i in range(1, 8)]\n",
        "llmchat(\"123\",images=image_paths)"
      ],
      "metadata": {
        "id": "6uzgDV54Mv3s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_paths = [f\"/content/receipt{i}.jpg\" for i in range(1, 8)]\n",
        "llmchat(\"How much money did I spend in total for these bills?\",images=image_paths)"
      ],
      "metadata": {
        "id": "iuuy-s-WN86H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_paths = [f\"/content/receipt{i}.jpg\" for i in range(1, 8)]\n",
        "llmchat(\"How much would I have had to pay without the discount?\",images=image_paths)"
      ],
      "metadata": {
        "id": "KLyvvMQ8PNM4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}